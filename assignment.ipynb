{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Word counting\n",
    "---------------\n",
    "\n",
    "For the first part of the practical assignment, we will use MapReduce paradigms to implement a word counting program in Apache Spark. First, some setup and data preparation. For further information on the software, check out the documentation [here](https://spark.apache.org/docs/latest/api/python/pyspark.html).\n",
    "\n",
    "To run the code in a cell, select it and press Ctrl+Enter, or use the `Run` button at the top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext(\"local[*]\", \"PUC Big Data workshop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the text file containing Shakespeare's famous Romeo & Juliet using a convenient PySpark function. This will automatically split the text file into separate lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give you an idea of the contents of this file, we will print a few random lines. The `takeSample` operation can be very useful to examine the data you are working with; particularly when working with a large amount of data, it is unfeasible to look at it all, so looking at a random small subset can give you a rough idea of what you are working with. Feel free to sample the results of the individual cells below to see what is happening!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  Who, but for dreaming on this fond exploit,',\n",
       " '',\n",
       " \"  And cherish factions. 'Tis inferr'd to us  \",\n",
       " \"SECOND MURDERER. 'Tis better, sir, than to be tedious. Let\",\n",
       " \"  Back'd by the power of Warwick, that false peer,\",\n",
       " \"  him today, I can tell them that. And there's Troilus will not\",\n",
       " '  A Roman sworder and banditto slave',\n",
       " 'RIANA. Say, how grows it due?',\n",
       " '  You did mistake.',\n",
       " 'If by strong hand you offer to break in']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.takeSample(withReplacement=False, num=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since some lines in the file are empty, we first filter those out; they do not contain words, so we don't need them! The `filter` operation will give us all lines with a length larger than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_empty_lines = lines.filter(lambda line: len(line) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extensively use `lambda` functions when working with Spark, since they allow for a nice and short notation of simple operations. Usually, the function will take one argument and perform some action on that argument. For example, the function we just used takes a single line, and checks whether its length is larger than 0 (e.g. it is not empty)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of non-empty sentences, we will split these sentences into single words. Since this is just an exercise, we will simply split the sentences on space characters; this will not give a perfect split, but it is good enough to use for the rest of the program. `flatMap` will make sure that we don't end up with nested lists and instead just give us one long list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = non_empty_lines.flatMap(lambda line: line.strip().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `strip` function removes leading and trailing spaces from the line, and the `split` function splits the line on spaces.\n",
    "\n",
    "Let's look at some of the words we ended up with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thou',\n",
       " \"'Item.\",\n",
       " 'lay',\n",
       " 'Cold',\n",
       " 'her',\n",
       " 'should',\n",
       " 'from',\n",
       " 'OFFICERS',\n",
       " 'than',\n",
       " 'deeps;']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.takeSample(withReplacement=False, num=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loaded the text of Shakespeare's famous Romeo & Juliet, removed empty lines and split the remaining lines on spaces using the `flatMap` function. Displayed above are 25 random words sampled from the split text. As you can see, splitting the sentences on spaces does not result in a perfect separation of words but it will do for our purposes.\n",
    "\n",
    "------------------------\n",
    "\n",
    "Now, let's implement a simple word count! First, we will use the `map` operation to transform each word into a (word, 1) tuple as per the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the dots with the right code:\n",
    "annotated_words = words.map(lambda word: ...)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After annotating each word with the number 1, we can then perform the shuffle step by using the `groupByKey` operation. This will group all (word, 1) tuples with identical keys (words) to the same worker node, and transform the (word, 1) tuples into (word, [1, 1, ...]) tuples, with all the 1's for a single word grouped into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_words = annotated_words.groupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the total word counts, we simply need to add up all the 1's for each word. We can do that by using the `mapValues` operation. This will pass all the 1's in each (word, [counts]) tuple into the sum function, so we end up with (word, totalCount) tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the dots with the right code!\n",
    "# Hint: look at the builtin Python sum function:\n",
    "# https://docs.python.org/3/library/functions.html#sum\n",
    "word_counts = grouped_words.mapValues(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to make the next steps easier, we first swap the positions of the words and their counts in the tuples so we end up with (count, word) instead of (word, count). This makes the count the key of the item, and will allow us to sort by key to see which words are most common. We need to do this because Spark does not allow us to sort by values, only by keys.\n",
    "\n",
    "To access the individual values in a tuple, you can use the bracket syntax:\n",
    "    \n",
    "```python\n",
    ">>> x = (\"hello\", 1)\n",
    ">>> x[0]\n",
    "\"hello\"\n",
    ">>> x[1]\n",
    "1\n",
    ">>> (x[1], x[0])\n",
    "(1, \"hello\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the dots with the right code:\n",
    "word_counts_reordered = word_counts.map(lambda x: ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can sort the tuples by their counts in descending order, putting the most frequent words at the beginning of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_word_counts = word_counts_reordered.sortByKey(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Let's look at the top 10 most frequent words in Romeo & Juliet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the : 23178  occurrences\n",
      "I   : 19540  occurrences\n",
      "and : 18218  occurrences\n",
      "to  : 15592  occurrences\n",
      "of  : 15503  occurrences\n",
      "a   : 12513  occurrences\n",
      "my  : 10823  occurrences\n",
      "in  : 9564   occurrences\n",
      "you : 9058   occurrences\n",
      "is  : 7829   occurrences\n"
     ]
    }
   ],
   "source": [
    "for (count, word) in sorted_word_counts.take(10):\n",
    "    # print the word following by its count\n",
    "    print(f\"{word:<4}: {count:<6} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Well done!\n",
    "That's it for the first part of this assignment. After the next part of the lecture, we can continue with part 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Averaging a list of numbers\n",
    "---\n",
    "In this part of the assignment, we will tackle a slightly more complicated problem; averaging a list of numbers. As in the first part, we will first create the data we need to get started. We will use a list of ten thousand random integers between 0 and one million. For reference, we calculate the average first using `numpy` functionality and then try to replicate it using Spark!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499656.4287\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "numbers = numpy.random.randint(0, 1_000_000, size=10_000)\n",
    "print(numbers.mean())\n",
    "\n",
    "# Parallelize the list to use it with Spark after this cell\n",
    "numbers = sc.parallelize(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try the same strategy we used for word counting. Transform the list of numbers into (number, 1) tuples using the `map` function. Then use the `groupByKey` function to collect all 1's for each number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_numbers = numbers.map(lambda x: (x, 1))\n",
    "grouped_numbers = annotated_numbers.groupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in part 1, we now have (number, [1, 1, ...]) tuples. Now to calculate the average of all the numbers, we can sum the 1's per number to get its occurrences. Then we divide each number by its number of occurrences, and sum all those results in order to get our answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498961.2909601686\n"
     ]
    }
   ],
   "source": [
    "# sum all the 1's\n",
    "number_counts = grouped_numbers.mapValues(sum)\n",
    "# divide each number by its count\n",
    "single_averages = number_counts.map(lambda x: x[0] / x[1])\n",
    "# sum all local averages\n",
    "average = single_averages.sum() / single_averages.count()\n",
    "\n",
    "print(average)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
